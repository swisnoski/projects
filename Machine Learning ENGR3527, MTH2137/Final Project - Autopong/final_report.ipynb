{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sam Wisnoski\n",
    "# Reinforcement Learning with Open-AI Gym - Machine Learning FA2024\n",
    "### Project Overview: For my final project, I will have two fully integrated Reinforcement Learning models: one for pong and one for a \"walking\" bidel robot. For each of these models, I will be using a combination of pre-built Open-AI gym enviornments and homebuilt algorithms. \n",
    "\n",
    "## Table of Contents: \n",
    "#### I. Project Information\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. Project Deliverables \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Key Resources\n",
    "\n",
    "#### II. Background Infomation\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. What is Reinforcement Learning (RL)?\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Why Do I Care About RL? \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C. Why Should Others Care About RL?\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D. Other Context and Ethics\n",
    "\n",
    "\n",
    "#### III. Pong with a Prebuilt Model\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. Initializing the Pong Enviornment\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Building and Training the Prebuilt Model  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C. Model Analysis \n",
    "\n",
    "#### IV. Pong with Tensorflow Model\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. Model Overview\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Defining Helpful Functions \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C. Building and Training our Tensorflow Model\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D. Model Analysis \n",
    "\n",
    "#### V. Bipedal Walker with Tensorflow Model \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. A Guilty Confession...\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Transitioning the Model from Pong to BipedalWalker\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; C. Building our Tensorflow Model\n",
    "\n",
    "#### VI. Project Conclusions and Reflection\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; A. Conclusions\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; B. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Project Information\n",
    "\n",
    "### A. Project Deliverables \n",
    "\n",
    "#### Learning Goals: \n",
    "1. I want to understand the theory and implementation of reinforcement learning. I want to implement RL both using pre-existing code libraries (RLlib) and attempt to implement it using my own functions/classes. \n",
    "2. I want to explore the application of reinforcement learning in the context of controlling video game agents.\n",
    "3. I want to explore the application of reinforcement learning in the context of automated robotics.  \n",
    "4. I want to explore different levels of challenging environments provided by OpenAI Gym and train at least two agents in different environments.\n",
    "5. I want to complete an appropriately scoped project and be content with the final product and my overall learning/understanding of the project topics\n",
    "6. (Stretch Goal) I want to use CNNs to analyze video game states and train a RL algorithm based on the output of a CNN model with pixel input \n",
    " \n",
    "\n",
    "#### Technical Deliverables:\n",
    "1. I want to have two fully integrated RL models: one for pong and one for the bipedal walker. \n",
    "2. I want to have built a RL model from scratch and train OpenAI Gym environments. \n",
    "\n",
    "#### Presentable Deliverables:\n",
    "1. I want to have a Jupyter notebook for pong and bipedal_walker that gives a detailed explanation of the code and implementation. Additionally, this will give a more in-depth overview of the context & ethics of my project. \n",
    "2. I want to create a poster that covers the basics of implementation, theory, ethics, context, and real world use of RL. The poster should be easily digestible. \n",
    "\n",
    "\n",
    "### B. Key Resources\n",
    "\n",
    "#### OpenAI Gymnasium:\n",
    "OpenAI Gymnasium is an updated toolkit (updated from OpenAI Gym) designed for developing reinforcement learning algorithms. It provides a variety of pre-defined environments, including simulations for games (pong) and robotics (bipedal walker), which will allow me to train and test my RL agents effectively. It provides key RL aspects such as frames, state variables, and rewards. For my test/baseline model, I use OpenAI Gymnasium in conjunction with the stable_baselines3 library to train a Deep Q Network model. \n",
    "\n",
    "Sources: \n",
    "* https://gymnasium.farama.org/\n",
    "* https://gymnasium.farama.org/environments/box2d/bipedal_walker/\n",
    "* https://ale.farama.org/environments/pong/ \n",
    "\n",
    "\n",
    "#### TensorFlow:\n",
    "TensorFlow is a powerful machine learning framework that I will use for building, training, and deploying my Deep Q RL model. It functions very similarly to Pytorch. I chose to Tensorflow over Pytorch given that Tensor flow has slightly better performance/optimization, and my models require training for many many hours. \n",
    "\n",
    "Sources: \n",
    "* https://www.tensorflow.org/agents/tutorials/0_intro_rl \n",
    "* https://www.geeksforgeeks.org/implementing-deep-q-learning-using-tensorflow/ \n",
    "\n",
    "\n",
    "#### ChatGPT:\n",
    "I wanted to credit ChatGPT as a key resource here as I found myself visiting it frequently when attempting to integrate pretty much every part of this project. The documentation for OpenAI Gymnasium is outdated and although I tried for several hours, seemingly does not work in a google colab notebook. I eventually got it running in a python file, and actually trained the first model completely outside of this notebook. Additionally, it took a long time (and a lot of googling) in order to run my tensorflow model on my gpu using cuda instead of my cpu. For both of these processes, I tried searching other resources first (friends, the general internet, stackoverflow), but could not find good solutions to my problems. For these reasons, I turned to ChatGPT for help. \n",
    "\n",
    "Notably, I did not use ChatGPT to assist with building the model, nor for any of the writeup or data analysis. \n",
    "\n",
    "Sources:\n",
    "* https://chatgpt.com/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background Infomation\n",
    "\n",
    "### A. What is Reinforcement Learning (RL)?\n",
    "\n",
    "Reinforcement learning is a machine learning technique focused on action based decision making and feedback loops. Unlike supervised learning, where a model learns from labeled data, reinforcement learning involves an agent that learns by interacting with its environment. The agent receives feedback in the form of rewards or penalties based on its actions and adjusts its strategy to maximize rewards.\n",
    "In our case, our RL model will consist of two parts: An environment, which will provide frame/state data, and an agent, which can take actions to interact with the environment. Based on those actions, the environment will provide it with positive or negative feedback.\n",
    "\n",
    "At its core, reinforcement learning mimics how humans learn through trial and error. It balances exploration (trying new actions) and exploitation (using known strategies to achieve rewards), which allows RL models to optimize behaviors in complex, dynamic environments. This makes RL uniquely suited for solving problems where the best solution cannot be predetermined or hard-coded (video games, robotics… but more on specific fields later). \n",
    "\n",
    "\n",
    "### B. Why Do I Care About RL? \n",
    "\n",
    "I wanted to embark on this project for a number of reasons. \n",
    "\n",
    "For starters, I can’t stress enough how much I love video games. Video games defined my youth. I grew up playing Super Mario Galaxy 2 on the Wii with my older brother, and I have countless memories of playing Minecraft, Terraria, or Valheim with my friends. There’s so much I love about video games: the stories they tell, the collaboration they foster, and the thrill of slowly unraveling the key mechanics and strategies of a game. That’s what makes this project so exciting. Each game has its own unique path to victory.\n",
    "\n",
    "I’ve spent days studying boss mechanics, optimizing my gear, or figuring out which items I needed to craft a specific upgrade. With so many different types of video games, condensing an optimal strategy for all of them would be impossible. But generally, success involves exploration—whether it’s exploring your abilities, the map, or interactable objects—and then reacting to what you discover.\n",
    "\n",
    "At a high level, this process is fundamentally what reinforcement learning is. Reinforcement learning involves exploration, receiving feedback in the form of rewards, and then adjusting its actions based on those rewards. It takes the very human task of trial, feedback, and learning and uses it to build an optimal model.\n",
    "\n",
    "For training our Pong model, our “opponent” is literally hard-coded to follow the ball’s y-position, ensuring that the paddle is always in the right spot. It’s designed to be unbeatable. However, as you’ll see, our algorithm still finds a way to outsmart this system. Given enough time to explore and learn, it discovers that it can score points by “spiking” the ball downward, giving it enough momentum to bypass the hard-coded paddle.\n",
    "\n",
    "I’m amazed by many different aspects of Machine Learning: predicting data, analyzing text, and generating images—but reinforcement learning stands out to me because it’s inherently human and exploratory. It’s not just about memorizing patterns or crunching numbers; it’s about adapting and discovering strategies in real-time, much like how we learn and grow through experience. This ability to explore and optimize solutions makes reinforcement learning uniquely powerful and deeply fascinating—and also, again, I just love video games :) \n",
    "\n",
    "\n",
    "### C. Why Should Others Care About RL?\n",
    "\n",
    "Reinforcement learning isn’t just a tool for training game-playing algorithms or building smarter AI systems—it’s a framework for solving complex, real-world problems. At its core, it mirrors how humans learn: through trial, error, and adapting to feedback. This makes it an incredibly versatile approach to tackling challenges that are too dynamic or unpredictable for traditional programming methods.\n",
    "\n",
    "Take robotics, for example. Robots trained with reinforcement learning can learn to walk, grasp objects, or navigate complex environments without being explicitly programmed for every scenario. In healthcare, it can optimize personalized treatments or design efficient care plans. In industries like logistics, reinforcement learning can find better ways to allocate resources or optimize delivery routes in real time.\n",
    "\n",
    "Let’s look at the second part of my project, OpenAI’s bipedal walker environment. While it heavily simplifies the problem of bipedal robots to a basic simulation with four magically controlled joints, it still presents a fascinating challenge. The walker must learn to balance, move forward efficiently, and adapt to changes in terrain—all without being given explicit instructions on how to do so, simply taking a reward. Now expand this to a real world robot that we could model. With the proper physics engine and enough computing power, we could use reinforcement learning to teach the robot how to walk… or pick up objects, or open doors, or do any number of complex tasks. The bipedal walker is just one of the kinds of problems reinforcement learning can tackle in robotics, and robotics is just one example of a field where reinforcement learning can be applied.\n",
    "\n",
    "Even in areas that seem far removed from gaming or tech, reinforcement learning has a role to play. It’s being used to address sustainability challenges, such as optimizing energy grids or developing strategies for climate modeling. The potential applications are practically endless, ranging from individualized education to autonomous systems to scientific discovery. RL can be used in any system that requires adapting to unpredictable and ever-changing environments.\n",
    "\n",
    "Ultimately, reinforcement learning is more than just an exciting tool for problem-solving or video games—it’s a bridge between automated creativity and technology. It allows computers to approach challenges with curiosity, adaptability, and a willingness to try and fail and try again. Whether it’s making video games more dynamic, helping robots walk, or solving one of humanity’s many other pressing issues, reinforcement learning has the potential to reshape the way we interact with the world. \n",
    "\n",
    "\n",
    "### D. Other Context and Ethics\n",
    "\n",
    "Reinforcement learning, like any powerful technology, raises important ethical considerations. One major concern is how reinforcement learning models handle biases in their reward systems. If a poorly designed reward system reinforces harmful behaviors, the agent may optimize for unintended or unethical outcomes. For example, in autonomous systems like self-driving cars, misaligned incentives could lead to decisions that prioritize efficiency over safety or equity. Who chooses the rewards when training models?—I’m not sure I’d trust the car company, which aims to maximize profit. \n",
    "\n",
    "Moreover, RL systems can sometimes develop behaviors that are difficult to interpret or predict; a trained model can have millions of tuned parameters that are impossible to fully understand. This \"black-box\" nature creates challenges when these systems are deployed in high-stakes environments, such as healthcare or military applications, where accountability and transparency are critical. For example, if an RL-trained system recommends a treatment plan that doesn’t align with established medical guidelines, how do we determine whether it found a genuinely better solution or if it misinterpreted the data?\n",
    "\n",
    "In the military, reinforcement learning is already being explored for applications like autonomous drones and simulations of battlefield strategies. Here, the stakes are even higher—unexpected behaviors in a combat situation could have catastrophic consequences. A reinforcement learning algorithm tasked with \"neutralizing threats\" might learn unintended and unethical behaviors based on its training environment or the incentives encoded in its reward system. Again, I’m not sure I trust the military to properly encode humanitarian values into the rewards systems.\n",
    "\n",
    "However, it’s obviously not all scary and bad. RL has also demonstrated remarkable potential in solving real-world problems and improving lives. A great example is in renewable energy optimization. RL algorithms are currently being used to manage energy grids by dynamically balancing supply and demand in real time. For instance, Google's DeepMind has applied reinforcement learning to control heating, cooling, and lighting based on occupancy patterns, reducing energy consumption by 40% in its data centers. Reinforcement learning is also being applied to agriculture, where it is helping to optimize resource usage and improve crop yields—the reward is crop yield, and the action can be any number of agricultural tactics. \n",
    "\n",
    "As it is with all technology, Reinforcement Learning is just another tool for humanity to use, and it is up to us to decide how to use it. As Richard Feynman once said: “Every man is given the key to the gates of heaven. The same key opens the gates of hell. And so it is with science.”\n",
    "\n",
    "Sources: \n",
    "* https://neptune.ai/blog/reinforcement-learning-applications#:~:text=Various%20papers%20have%20%EE%80%80proposedDeep%20Reinforcement \n",
    "* https://medium.com/@hassaanidrees7/reinforcement-learning-in-real-world-applications-from-theory-to-practice-2f67a6f673cb \n",
    "* https://medium.com/@mlblogging.k/9-awesome-applications-of-reinforcement-learning-e1306ed25c09 \n",
    "* https://www.goodreads.com/quotes/421467-to-every-man-is-given-the-key-to-the-gates \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Pong with a Prebuilt Model\n",
    "\n",
    "### A. Initializing the Pong Enviornment\n",
    "\n",
    "#### What is Pong? How will we traing it?\n",
    "\n",
    "Now that we have a good idea of what Reinforcement Learning is, let's move on to creating our first OpenAI enviornment! There's just one last thing before we begin: let's talk about the game of Pong. \n",
    "\n",
    "Pong is one of the first video games to have been created. It is a simple two player game that mimics an air hockey table. There are only ever three items on the screen- each players \"paddle\", which can move vertically, and the \"ball\", which can bounce off the walls and paddles. The goal of the game is to hit the ball and prevent it from moving past your paddle. To learn more about Pong and the enviornment we will be using, you can visit: https://ale.farama.org/environments/pong/ \n",
    "\n",
    "To train pong, we will be using a Deep Q Learning network, which will take frames of the enviornment (processed through a CNN) and a set of actions. It will return a list of information, including the next game state and a reward. Using the given game states and actions, our DQN will attempt to optimize actions for a Pong to maximize the reward. \n",
    "\n",
    "More explicity, our model variables are:\n",
    "* **Game State:** A frame of pong, which will be processed via CNN\n",
    "* **Actions:** Moving the left paddle up or down \n",
    "* **Reward:** A score from -21 to 21. Each point the hardcoded algorithm scores is a -1 score, and each point our algorithm score is +1. Games are played first to 21. \n",
    "\n",
    "\n",
    "#### Loading Dependancies\n",
    "\n",
    "Now, we are finally ready to begin model development. As always, the first step to creating our model is to install our required dependancies. \n",
    "\n",
    "Here is a short description of each dependency and why we need it: \n",
    "* import os: Provides functions to interact with the operating system.\n",
    "\n",
    "* **import gymnasium as gym:** Toolkit for developing and testing RL algorithms using standardized environments (e.g., Pong, Bipedal Walker).\n",
    "* **import ale_py:** Interfaces with classic Atari games (e.g., Pong) in Gymnasium environments\n",
    "* **from stable_baselines3 import DQN:** Imports DQN (Deep Q-Learning), the algorithm we will use to train our example pong enviornment.\n",
    "* **from stable_baselines3.common.vec_env import DummyVecEnv:** Creates a vectorized environment wrapper for compatibility with RL algorithms.\n",
    "* **from stable_baselines3.common.monitor import Monitor:** Logs training data (e.g., rewards, episode lengths) for evaluation.\n",
    "* **from stable_baselines3.common.evaluation import evaluate_policy:** Evaluates trained policies by running multiple episodes and calculating average rewards.\n",
    "* **import matplotlib.pyplot as plt:** Used to visualize training progress and other important metrics.\n",
    "* **from IPython.display import clear_output:** Clears cell output in Jupyter Notebooks for dynamic updates during training.\n",
    "* **import numpy as np:** Performs numerical computations and array operations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intitializing our Enviornmentl:\n",
    "The next step in our model creation is to initialize our Gym environment. \n",
    "\n",
    "Luckily, this step is actually pretty easy using OpenAI Gym. We simply need to make the enviornment and wrap it in a vectorized wrapper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return Monitor(gym.make(\"ALE/Pong-v5\"))\n",
    "\n",
    "env = DummyVecEnv([make_env])  # Wrap the environment with a vectorized wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Building and Training the Prebuilt Model. \n",
    "\n",
    "#### Initializing our DQN Model\n",
    "\n",
    "With a prebuilt model, (provided by stable_baselines3), initialization is also easy - all we need to do it call the DQN function. However, we want to use a specific combination of learning parameters to tune our model. I've tuned the prebuilt model take longer to train, but to be more effective over a long period of time. Here is a brief overview of what the each of the hyperparameters does: \n",
    "* **Learning Rate:** A smaller learning rate ensures slower, more stable updates, preventing the model from overshooting optimal solutions.\n",
    "* **Buffer Size:** A larger replay buffer allows the agent to sample from a wider range of experiences, improving generalization.\n",
    "* **Learning Starts:** Delaying the start of learning provides more time for exploration before the model begins to learn from its experiences.\n",
    "* **Batch Size:** A larger batch size provides smoother updates, improving training stability at the cost of increased computation.\n",
    "* **Target Update Interval:** Less frequent updates to the target network help maintain stability in the learning process.\n",
    "* **Exploration Fraction:** A higher exploration fraction encourages more random actions over a longer period, enhancing exploration.\n",
    "* **Exploration Final Epsilon:** Keeping exploration ongoing even after exploitation starts helps the agent avoid premature convergence to suboptimal policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with appropriate hyperparameters\n",
    "model = DQN(\n",
    "    \"CnnPolicy\",  # Neural network policy for image input\n",
    "    env,\n",
    "    learning_rate=1e-4,  # Learning rate for stability\n",
    "    buffer_size=5000,  # Replay buffer size\n",
    "    learning_starts=1000,  # Start learning after some steps\n",
    "    batch_size=32,  # Mini-batch size for updates\n",
    "    target_update_interval=500,  # Target network update frequency\n",
    "    exploration_fraction=0.1,  # Fraction of timesteps for exploration\n",
    "    exploration_final_eps=0.02,  # Final exploration rate (epsilon)\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard_logs/\",  # Enable TensorBoard logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Untrained Model\n",
    "\n",
    "Before we train our model, we want to record some statistics of our model pre-training. For visualization, let's play a test game with 300 game states to see how our model performs with completely random inputs. For the test game, it is important to note that the player on the left is a hardcoded agent that simply tracks the verticle position of the ball and moves the paddle to that position accordingly. The player on the left is controlled by our agent (which again, is currently just random inputs). \n",
    "\n",
    " Then, we will generate the average score (reward) of 10 games. We can directly use these scores to compare to our model post training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "env_random = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "env_random.action_space.seed(42)\n",
    "\n",
    "observation, info = env_random.reset(seed=42)\n",
    "\n",
    "\n",
    "for _ in range(300):\n",
    "    observation, reward, done, info, info2 = env_random.step(env_random.action_space.sample())\n",
    "\n",
    "    if done:\n",
    "        observation, info = env_random.reset(return_info=True)\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow( env_random.render())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scores for 5 Test Games\n",
    "for episode in range(1, 11):\n",
    "    score = 0\n",
    "    state = env_random.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = env_random.action_space.sample()\n",
    "        n_state, reward, done, info, more_info = env_random.step(action)\n",
    "        score += reward\n",
    "\n",
    "    print('Episode:', episode, 'Score:', score)\n",
    "env_random.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get scores between -21 and -19. A score of -21 is the lowest score possible and means that our player scored 0 points while the hard-coded opponent scored 21 points against us. A score of -19 means that our player scored 0 points while the hard-coded opponent scored 21 points against us. Obviously, these score our not good, ie., our untrained random model sucks. \n",
    "\n",
    "\n",
    "#### Training the Model\n",
    "Next, we will train the model. For our prebuilt model, this is as simple as defining the total number of timesteps and using the function \"model.learn()\". For the sake of computation time, I have commented out the code to train the model and simply am loading a model I previously trained over 10,000,000 timesteps. It took roughly 36 hours to train the model, and after training, the logs which I had been attempting to track became corrupted and I was unable to plot their data. However, I know that the model stagnated at its current point around 7,000,000 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (in theory)\n",
    "# total_timesteps = 10000000  # Total training timesteps\n",
    "# log_interval = 2  # Log every 10 training updates\n",
    "# model.learn(total_timesteps=total_timesteps, log_interval=log_interval)\n",
    "\n",
    "# # Save the trained model\n",
    "# model.save(\"pong_dqn\")\n",
    "# print(\"Model saved as 'pong_dqn.zip'.\")\n",
    "\n",
    "# Load the trained model\n",
    "model = DQN.load(\"pong_dqn_updated_2.zip\")\n",
    "\n",
    "env = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model Analysis\n",
    "\n",
    "\n",
    "Next, we want to evaluate how well our model became trained. To be consistent, let's generate the same metrics as our untrained model. First, lets play another test game and see how our model performs. We will play this game to completion, until either our ai agent or the hardcoded ai wins. (Reminder: our agent is on the left!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "obs, info = env.reset()  # Unpack the tuple\n",
    "\n",
    "for _ in range(5000):\n",
    "    # Predict the next action\n",
    "    action, _states = model.predict(obs)\n",
    "    \n",
    "    # Take the action in the environment\n",
    "    obs, rewards, done, info, more_info = env.step(action)\n",
    "    \n",
    "    # If the episode is done, reset the environment\n",
    "    if done:\n",
    "        break  # Reset with return_info=True\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won! Although it changes each time we reset the notebook, we usually win this game by around a margin of 5-10. This means we reach 21 points first, but we still allow the hard-coded agent to score on us between 11 and 16 times. \n",
    "\n",
    "To confirm these results, lets generate another 10 games using the same method as before and observing the change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model for 11 episodes\n",
    "for episode in range(1, 11):  # Test 10 episodes\n",
    "    obs, info = env.reset()  # Reset the environment at the start of each episode\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)  # Predict action based on current state\n",
    "        obs, reward, done, info, more_info = env.step(action)  # Take step in environment\n",
    "        score += reward  # Accumulate score over the episode\n",
    "\n",
    "    print(f\"Episode {episode} finished. Total Score: {score}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We a wider range of scores, but all positive! Again, these scores are re-generated each time the notebook is run, so there will be some variation (possibly even the rare negative score), but it's clear that there is a solid increase over these scores. On average, our RL trained agent beats the hard-coded bot by anywere from 5 to 15 points. \n",
    "\n",
    "Obviously, this can only mean one thing: Our pretrained model is a success!\n",
    "\n",
    "Now, let's step up the difficults and try something a little harder: building our own model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Pong with TensorFlow\n",
    "\n",
    "Now, we get to build our own model! Yay! Tensorflow! As a reminder, we will be using a Deep Q Learning network, which can take game state information (frames), as well as a set of actions (paddle up, paddle down, paddle stay), and return a list of information, including a reward. Using the given game states and actions, our model will attempt to optimize actions for a given game state to maximize the reward. \n",
    "\n",
    "\n",
    "## A. RL Model Overview\n",
    "\n",
    "#### References and Sources\n",
    "\n",
    "Before we begin coding, let's provide an higher-levle overview of the model. The model I ultimately chose to develop is largely from the paper \"Getting an AI to play atari Pong, with Deep Reinforcement Learning\" by Drew Parmelee. This paper can be found at https://towardsdatascience.com/getting-an-ai-to-play-atari-pong-with-deep-reinforcement-learning-47b0c56e78ae. \n",
    "\n",
    "I made the thoughtful decision to further develop and fine-tune an existing model, rather than create a new model entirely from scratch, for several key reasons. Reinforcement learning is an inherently complex domain that demands a deep understanding of both theoretical concepts and knowledge of frameworks like TensorFlow or PyTorchpractical for implementation. Drew Parmelee's paper offered a well-documented and effective approach to solving the Pong challenge using Deep Q-Learning. Additionally, this paper was not my only resource; I also invested significant time researching reinforcement learning to fully grasp the underlying mechanics and implementation details. This foundation the paper provided me was crucial in tackling the creation of a model from scratch for the Bipedal Walker environment later in this notebook.\n",
    "\n",
    "Leveraging large sections of code from the paper provided several important benefits:\n",
    "\n",
    "* **Save Development Time:** Reinventing the wheel would have been both time-consuming and error-prone. Using the paper as a reference allowed me to focus on understanding and fine-tuning the model, rather than troubleshooting low-level issues.\n",
    "* **Learn Through Iteration:** Modifying and adapting the code gave me valuable hands-on experience with the intricacies of deep reinforcement learning. This approach enabled me to experiment with alternative methods and improvements/modifications to the model. The model was built on an old version of Pong, and I needed to update the model to the latest version by rewriting several functions.\n",
    "* **Ensure Reliability:** By building on a trusted implementation, I minimized the risk of critical errors that could have disrupted training or evaluation. The existing model provided a solid baseline to build upon and optimize.\n",
    "\n",
    "Other referenced sources: \n",
    "* https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc \n",
    "* https://www.geeksforgeeks.org/deep-q-learning/ \n",
    "* https://arxiv.org/pdf/1312.5602 \n",
    "* https://medium.com/@liyinxuan0213/step-by-step-double-deep-q-networks-double-dqn-tutorial-from-atari-games-to-bioengineering-dec7e6373896 \n",
    "* https://www.analyticsvidhya.com/blog/2021/02/understanding-the-bellman-optimality-equation-in-reinforcement-learning/ \n",
    "* https://www.datacamp.com/tutorial/bellman-equation-reinforcement-learning \n",
    "\n",
    "\n",
    "#### How does a Reinforcement Learning Model work? \n",
    "\n",
    "The key components of an RL model are:\n",
    "\n",
    "1. **Environment:** The setting in which the agent operates (e.g., Pong game). The agent observes the current state of the environment and interacts with it by taking actions.\n",
    "2. **Agent:**: The decision-making entity that learns through trial and error. It chooses actions based on the current state and receives feedback in the form of rewards.\n",
    "3. **States:** Representations of the environment at a given time. In Pong, states are frames of the game processed through a Convolutional Neural Network (CNN).\n",
    "4. **Actions:** Possible moves the agent can make. In Pong, these are: moving the paddle up, moving it down, or keeping it stationary.\n",
    "5. **Rewards:** Numerical feedback that guides the agent's learning. In Pong, a reward of +1 is given for scoring a point, and -1 for losing a point.\n",
    "6. **Policy:** A strategy that maps states to actions. The agent learns the policy to maximize its cumulative reward.\n",
    "7. **Q-Learning:** A value-based method in reinforcement learning. It uses the Q-value function, which estimates the expected cumulative reward of taking an action in a given state and following the optimal policy thereafter. Deep Q-Learning (DQN) combines Q-Learning with deep neural networks. Instead of maintaining a table of Q-values, the agent uses a neural network to approximate the Q-value function for all possible state-action pairs.\n",
    "\n",
    "\n",
    "The implementation of the RL model in Drew Parmelee’s paper adapts Deep Q-Learning to train an AI agent to play Pong. Below, I’ll explain the specific steps and components of the model as described in the paper:\n",
    "\n",
    "##### Enviornment and State Processing: \n",
    "\n",
    "The Pong game environment is simulated using OpenAI Gym.\n",
    "Raw game frames are cropped, converted to grayscale, and resized to a smaller resolution (84x84 pixels). This reduces input complexity and focuses on essential features. Frames are stacked (e.g., the last 4 frames) to give the agent temporal context, allowing it to recognize motion and dynamics in the game.\n",
    "\n",
    "##### Choosing an Action: \n",
    "\n",
    "The agent can choose one of three actions: move the paddle up, move the paddle down, or stay stationary.\n",
    "An epsilon-greedy strategy is used to balance exploration (random actions) and exploitation (choosing the best-known action based on the Q-values). Our epsilon value starts high, such that at the beginning of the model, all of the actions taken are random. This is very useful for collecting data to learn from. As the model progressed, the epsilon value slowly lowers, eventually reaching a stagnant value (usually 0.05). \n",
    "\n",
    "##### Rewarding our Actions: \n",
    "\n",
    "Rewards are directly tied to game outcomes: +1 for scoring a point, and -1 for losing a point. The cumulative reward over the game (up to 21 points) determines the agent’s performance.\n",
    "\n",
    "##### Deep Q-Network (DQN):\n",
    "\n",
    "After we take the state, action, and reward from the enviornment, we can feed that information into the Bellman equation to calculate our Q-values. A Q-value represents the expected total reward an agent can achieve by taking a specific action in a given state and following an optimal policy afterward. In other words, it answers the question: \"If I take this action right now, how good will it be in terms of rewards, both immediately and in the future?\"\n",
    "\n",
    "The Q-values are updated using the Bellman equation:\n",
    "\n",
    "![Local Image](equation.png)\n",
    "\n",
    "Where:\n",
    "\n",
    "* S is the current state, \n",
    "* A is the action taken, \n",
    "* R is the reward, \n",
    "* t is the timestep,\n",
    "* alpha is the learning rate, and\n",
    "* lamda is the discount factor.\n",
    "\n",
    "The Bellman equation is the life and soul of our model, as well as the foundation of how many other reinforcement learning models work. It helps the agent figure out the value of an action by combining two things: the reward it gets right away and the rewards it might earn in the future. This way, the agent doesn’t just focus on quick wins but also plans for what’s ahead.\n",
    "\n",
    "In a Deep Q-Network (DQN), the Bellman equation is used to improve the agent’s guesses about which actions are best. Over time, it teaches the agent to make smarter decisions by comparing what it expected to happen with what actually happened. This process helps the agent learn to play in a way that earns the most rewards overall.\n",
    "\n",
    "For more information about the Bellman equation, reference the sources linked above. \n",
    "\n",
    "\n",
    "##### Experience Replay:\n",
    "\n",
    "The agent stores \"experiences\" in a memory class. Each experience consists of:\n",
    "* Current state (s_t).\n",
    "* Action taken (a_t).\n",
    "* Reward received (r).\n",
    "* Next state (s_(t+1)).\n",
    "\n",
    "Experience Replay helps stabilize and improve training by storing past experiences (current state, action, reward, next state) in a memory buffer. Instead of training on consecutive experiences, random batches are sampled from the buffer, breaking correlations and ensuring the agent learns from diverse scenarios. This process reinforces rare events, improves data efficiency, and smooths learning by reducing fluctuations in Q-value updates. In Drew Parmelee’s model, this technique is critical for training the Pong agent effectively and ensuring stable performance over time.\n",
    "\n",
    "##### Secondary Neural Network:\n",
    "\n",
    "To stabilize learning, the implementation includes a secondary network (the target network). This network is periodically updated with the weights of the primary network.\n",
    "During training, the target network provides fixed Q-value estimates, preventing oscillations in learning.\n",
    "\n",
    "##### Optimization and Training:\n",
    "\n",
    "The model uses the Adam optimizer to minimize the mean squared error (MSE) between predicted Q-values and target Q-values. Training occurs over many episodes, with the agent improving its policy as it learns to predict the optimal actions for each state.\n",
    "\n",
    "Wooh! That was a lot of information. Now that we have the basis of how a RL model works, lets jump into our implementation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Defining Helpful Functions \n",
    "\n",
    "#### Implementing a Frame Processor \n",
    "\n",
    "To prepare the Pong game frames for our reinforcement learning model, we need to reduce their complexity whilst retaining critical gameplay information. To do this, we can evaluate each pixel in the frame individually and turn it into a numpy array using the \"resize_frame\" function. We resize the frame to 84x84 pixels, removing the scoreboard, and then grayscale the frame. This ensures that the input to our model is standardized and compact. \n",
    "\n",
    "To visualize this, we initialize a random Pong Enviornment, play out the first 25 steps, then display the result using matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_frame(frame):\n",
    "    frame = frame[30:-12,5:-4]\n",
    "    frame = np.average(frame,axis = 2)\n",
    "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
    "    frame = np.array(frame,dtype = np.uint8)\n",
    "    return frame\n",
    "\n",
    "env_random = gym.make(\"ALE/Pong-v5\", render_mode=\"rgb_array\")\n",
    "env_random.action_space.seed(42)\n",
    "\n",
    "obs_random, info = env_random.reset(seed=42)\n",
    "\n",
    "for _ in range(25):\n",
    "    obs_random, reward, done, info, info2 = env_random.step(env_random.action_space.sample())\n",
    "\n",
    "processed_frame = resize_frame(obs_random)\n",
    "\n",
    "\n",
    "# Display the frame\n",
    "plt.imshow(processed_frame, cmap='gray')\n",
    "plt.title(\"Processed Frame\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^^ An example processed frame ^^^^\n",
    "\n",
    "\n",
    "#### Implementing Memory\n",
    "\n",
    "Our next step is to create a \"Memory\" class that is used to store the experiences described in the \"Experience Replay\" step above. It uses a deque (a deque is very similar to a list, expect variable are more easily added or subtracted from either end of a deque) to efficiently store and manage a history of previous gamestates. Each stored \"experience\" includes a frame, the action taken, the reward received, and a flag indicating whether the episode has ended (next_frame_terminal).\n",
    "\n",
    "The add_experience method appends new experiences to the memory buffer. Using a deque automatically discards the oldest experience once our deques reach their maximum lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.frames = deque(maxlen = max_len)\n",
    "        self.actions = deque(maxlen = max_len)\n",
    "        self.rewards = deque(maxlen = max_len)\n",
    "        self.done_flags = deque(maxlen = max_len)\n",
    "\n",
    "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
    "        self.frames.append(next_frame)\n",
    "        self.actions.append(next_action)\n",
    "        self.rewards.append(next_frames_reward)\n",
    "        self.done_flags.append(next_frame_terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Functions to Connect OpenAI Gym and Our Model\n",
    "\n",
    "Next, we define four more functions to help integrate our model and the OpenAI gymnasium environment. Here is a brief description of each: \n",
    "\n",
    "**1. initialize_new_game:**\n",
    "\n",
    "This function prepares the environment and the agent's memory for a new game. It resets the game environment (env) and generates a starting frame.\n",
    "\n",
    "**2. make_env:**\n",
    "\n",
    "A simple function to create a new game environment.\n",
    "\n",
    "**3. take_step:**\n",
    "\n",
    "This function defines the logic for a single step in the environment:\n",
    "\n",
    "It first increments a single timestep (this is the model taking a single action/processing a single frame) for tracking progress. It periodically saves the model's weights as a checkpoint. Next, it executes an action using the agent’s most recent action and observe the environment’s response (new frame, reward, and whether the game ended). It then processes the frame and combines it with the last three frames from memory to create the next s. This is reshaped into the format expected by TensorFlow (e.g., normalized pixel values). After creating the experience, the agent’s policy to decide the next action based on the new state. If it isn't the last frame of the game, the experience is added to the model's memory. If the game ends, it returns the final score and a \"done\" status.\n",
    " \n",
    "Lastly, once we have enough random experiences (in this case, 50,000), we begin training, and traing based on every frame.\n",
    "\n",
    "**4. play_episode:**\n",
    "\n",
    "This function ties everything together to play one full game episode. It first initializes the game environment and agent memory, then enters a loop where it repeatedly calls take_step until the game is over. Lastly, the final score of the episode is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_new_game(name, env, agent):\n",
    "    \"\"\"\n",
    "    Initialize a new game and prepare the agent's memory.\n",
    "    We add dummy data to the agent's memory to ensure a consistent state\n",
    "    at the beginning of the game and avoid influence from past games.\n",
    "    \"\"\"\n",
    "    # Reset the environment to start a new game\n",
    "    env.reset()\n",
    "    \n",
    "    # Get the first frame after taking a dummy action (action 0)\n",
    "    starting_frame = resize_frame(env.step(0)[0])\n",
    "\n",
    "    # Define dummy experience parameters\n",
    "    dummy_action = 0\n",
    "    dummy_reward = 0\n",
    "    dummy_done = False\n",
    "    \n",
    "    # Add three dummy experiences to initialize the agent's memory\n",
    "    for i in range(3):\n",
    "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
    "\n",
    "\n",
    "def make_env(name):\n",
    "    \"\"\"\n",
    "    Create and return a new game environment with the specified name.\n",
    "    \"\"\"\n",
    "    # Create the game environment using OpenAI Gym\n",
    "    env = gym.make(name, render_mode=\"rgb_array\")\n",
    "    return env\n",
    "\n",
    "\n",
    "def take_step(env, agent, score):\n",
    "    \"\"\"\n",
    "    Perform a single step in the environment:\n",
    "    - Update the agent's timesteps and optionally save weights\n",
    "    - Execute an action and observe the next state, reward, and terminal status\n",
    "    - Update the agent's memory and trigger learning if memory threshold is reached\n",
    "    - Return the updated score and game-over status\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1 and 2: Increment total timesteps and periodically save model weights\n",
    "    agent.total_timesteps += 1\n",
    "    if agent.total_timesteps % 100000 == 0:\n",
    "        agent.model.save_weights('recent.weights.h5')\n",
    "        print('\\nWeights saved!')\n",
    "\n",
    "    # 3: Take the agent's most recent action and observe the environment's response\n",
    "    next_frame, next_frames_reward, next_frame_terminal, info, info2 = env.step(agent.memory.actions[-1])\n",
    "    \n",
    "    # 4: Process the next frame to create the next state\n",
    "    next_frame = resize_frame(next_frame)  # Resize and preprocess the frame\n",
    "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]  # Stack the last 4 frames\n",
    "    new_state = np.moveaxis(new_state, 0, 2) / 255  # Adjust shape to [rows, columns, channels] and normalize pixel values\n",
    "    new_state = np.expand_dims(new_state, 0)  # Add a batch dimension for model input\n",
    "    \n",
    "    # 5: Determine the next action based on the new state using the agent's policy\n",
    "    next_action = agent.get_action(new_state)\n",
    "\n",
    "    # 6: If the game is over, add the final experience to memory and return the score\n",
    "    if next_frame_terminal:\n",
    "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "        return (score + next_frames_reward), True\n",
    "\n",
    "    # 7: Add the new experience to the agent's memory\n",
    "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "\n",
    "    # 8: If memory buffer is sufficiently full, trigger the agent's learning process\n",
    "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
    "        agent.learn()\n",
    "\n",
    "    # Return the updated score and a \"not done\" status\n",
    "    return (score + next_frames_reward), False\n",
    "\n",
    "\n",
    "def play_episode(name, env, agent):\n",
    "    \"\"\"\n",
    "    Play a single game episode:\n",
    "    - Initialize the game and agent's memory\n",
    "    - Take steps in the environment until the game ends\n",
    "    - Return the total score achieved in the episode\n",
    "    \"\"\"\n",
    "    # Initialize the game and agent's memory\n",
    "    initialize_new_game(name, env, agent)\n",
    "    \n",
    "    # Play the game until it ends\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        score, done = take_step(env, agent, score)\n",
    "    \n",
    "    # Return the final score of the episode\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Building and Training our Tensorflow Model\n",
    "\n",
    "Now that we have our helper functions in place, we can use tensorflow to construct our model. This is done by creating an Agent Class that has functions that can build and train a model.\n",
    "\n",
    "#### Building the Model \n",
    "\n",
    "The Agent class is the core of our reinforcement learning setup. Below is an overview of the key components and methods of this class:\n",
    "\n",
    "**1. Initialization (__init__ method)** \n",
    "\n",
    "This method initializes the agent with the following attributes:\n",
    "\n",
    "* **memory:** A Memory object to store past experiences.\n",
    "* **possible_actions:** A list of all valid actions in the environment.\n",
    "* **epsilon:** The exploration rate for choosing random actions, decaying over time.\n",
    "* **epsilon_decay:** The rate at which epsilon decays with each episode.\n",
    "* **gamma:** The discount factor that determines how future rewards are valued.\n",
    "* **learn_rate:** Controls how much the model’s weights are updated during training.\n",
    "* **model and model_target:** Neural networks for the current policy and a separate target policy (used for stability).\n",
    "* **starting_mem_len and total_timesteps:** Counters for tracking learning progress and ensuring sufficient experience collection before training.\n",
    "\n",
    "**2. Building the Neural Network (_build_model method)**\n",
    "\n",
    "The _build_model method constructs the Deep Q-Network (DQN), which maps the agent's state (a sequence of frames) to Q-values for each possible action.\n",
    "\n",
    "Input Layer:\n",
    "* Shape: (84, 84, 4) – 84x84 resized grayscale frames, with 4 stacked frames providing temporal context to capture motion.        \n",
    "\n",
    "\n",
    "Three convolutional layers extract spatial and temporal features:\n",
    "* Layer 1: 32 filters, kernel (8, 8), stride 4 – detects large-scale features.\n",
    "* Layer 2: 64 filters, kernel (4, 4), stride 2 – captures finer details.\n",
    "* Layer 3: 64 filters, kernel (3, 3), stride 1 – refines small-scale features.\n",
    "\n",
    "Activation: \n",
    "* ReLU for non-linearity and efficient learning.\n",
    "\n",
    "Flatten and Dense Layers: \n",
    "* Flatten: Compresses the extracted features into a 1D vector.\n",
    "* Dense 1: 512 neurons, ReLU activation – processes complex feature relationships.\n",
    "* Output: Linear activation with one Q-value for each possible action.\n",
    "\n",
    "Loss Function:\n",
    "* Huber Loss: Balances sensitivity to errors and robustness to outliers, ideal for noisy reinforcement learning rewards.\n",
    "\n",
    "Optimizer:\n",
    "* Adam Optimizer: Adaptive and efficient, using the learning rate specified during agent initialization.\n",
    "\n",
    "**3. Deciding Actions (get_action method)**\n",
    "\n",
    "The get_action method implements the exploration-exploitation trade-off:\n",
    "\n",
    "With probability epsilon, the agent selects a random action (exploration).\n",
    "Otherwise, it selects the action with the highest Q-value predicted by the model (exploitation).\n",
    "\n",
    "**4. Validating Experiences (_index_valid method)**\n",
    "\n",
    "This helper function ensures that a batch of experiences does not contain incomplete transitions caused by terminal frames.\n",
    "\n",
    "**5. Learning from Experiences (learn method)**\n",
    "\n",
    "The learn method trains the agent to improve its policy by using stored experiences. Key steps include:\n",
    "\n",
    "Experience Sampling: Randomly samples a batch of 32 valid experiences from memory. Each experience includes a state, action, reward, next state, and whether the game ended.\n",
    "Target Q-Value Calculation: Combines the observed reward with the discounted maximum Q-value from the target model for the next state.\n",
    "Updating the Network: Trains the model using the states and computed Q-value targets as labels.\n",
    "Epsilon Decay: Gradually reduces exploration as the agent learns.\n",
    "Updating the Target Model: Periodically copies weights from the main model to the target model for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.possible_actions = possible_actions\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = .9/100000\n",
    "        self.epsilon_min = .05\n",
    "        self.gamma = .95\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input((84,84,4)))\n",
    "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
    "        print('\\nAgent Initialized\\n')\n",
    "        return model\n",
    "\n",
    "    def get_action(self,state):\n",
    "        \"\"\"Explore\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.sample(self.possible_actions,1)[0]\n",
    "\n",
    "        \"\"\"Do Best Acton\"\"\"\n",
    "        a_index = np.argmax(self.model.predict(state, verbose = 0))\n",
    "        return self.possible_actions[a_index]\n",
    "\n",
    "    def _index_valid(self,index):\n",
    "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"We want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
    "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
    "\n",
    "        # First we need 32 random valid indices\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions_taken = []\n",
    "        next_rewards = []\n",
    "        next_done_flags = []\n",
    "\n",
    "        while len(states) < 32:\n",
    "            index = np.random.randint(4, len(self.memory.frames) - 1)\n",
    "            if self._index_valid(index):\n",
    "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
    "                state = np.moveaxis(state, 0, 2) / 255\n",
    "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
    "                next_state = np.moveaxis(next_state, 0, 2) / 255\n",
    "\n",
    "                states.append(state)\n",
    "                next_states.append(next_state)\n",
    "                actions_taken.append(self.memory.actions[index])\n",
    "                next_rewards.append(self.memory.rewards[index+1])\n",
    "                next_done_flags.append(self.memory.done_flags[index+1])\n",
    "\n",
    "        # Get the outputs from our model and the target model for our target in the error function\n",
    "        labels = self.model.predict(np.array(states), verbose = 0)\n",
    "        next_state_values = self.model_target.predict(np.array(next_states), verbose = 0)\n",
    "\n",
    "        # Define our labels, or what the output should have been\n",
    "        for i in range(32):\n",
    "            action = self.possible_actions.index(actions_taken[i])\n",
    "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
    "\n",
    "        # Train our model using the states and outputs generated\n",
    "        states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        labels_tensor = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "        self.model.fit(states_tensor, labels_tensor, batch_size=64, epochs=1, verbose=0)\n",
    "\n",
    "        # Decrease epsilon and update how many times our agent has learned\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.learns += 1\n",
    "\n",
    "        # Every 10000 learns, copy our model weights to our target model\n",
    "        if self.learns % 1000 == 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            print('\\nTarget model updated')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "\n",
    "We have reached the end of model building! Time to train our model. First, we initialize the Pong enviornment and our Agent. We then initialize a loop to run for 50,000 episodes. While looping, we track total steps, duration, score- updating the maximum score as needed. After each episode, it prints these key metrics so we can see progression over time. \n",
    "\n",
    "Like the pre-built model, I have pre-trained this model over the course of 28 hours and will be loading those weights rather than retraining the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734628478.036541    1207 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3245 MB memory:  -> device: 0, name: NVIDIA RTX A1000 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Initialized\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swisnoski/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress all but errors\n",
    "\n",
    "# Set TensorFlow to use only CPU\n",
    "# tf.config.set_visible_devices([], 'GPU')  # Hides the GPU devices\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpus[0], False)  # Optionally disable GPU memory growth\n",
    "\n",
    "\n",
    "name = 'ALE/Pong-v5'\n",
    "\n",
    "agent = Agent(possible_actions=[0,2,3],starting_mem_len=50000,max_mem_len=250000,starting_epsilon = 1, learn_rate = .00025)\n",
    "env = make_env(name)\n",
    "\n",
    "scores = deque(maxlen = 100)\n",
    "max_score = -21\n",
    "\n",
    "\n",
    "env.reset()\n",
    "\n",
    "\"\"\"for i in range(50000):\n",
    "    timesteps = agent.total_timesteps\n",
    "    timee = time.time()\n",
    "    score = play_episode(name, env, agent)\n",
    "    scores.append(score)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "    print('\\nEpisode: ' + str(i))\n",
    "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
    "    print('Duration: ' + str(time.time() - timee))\n",
    "    print('Score: ' + str(score))\n",
    "    print('Max Score: ' + str(max_score))\n",
    "    print('Epsilon: ' + str(agent.epsilon))\"\"\"\n",
    "\n",
    "# For loading and testing a model:\n",
    "agent.model.load_weights('recent.weights.h5')\n",
    "agent.model_target.load_weights('recent.weights.h5')\n",
    "agent.epsilon = 0.00\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Model Analysis \n",
    "\n",
    "As mentioned above, our model trained for a total of 28 hours. This time, I actually recorded data so we could see the model's progression over time. First, I want to present some metrics generated during training; then, we can compare this model to the pre-built and random models. \n",
    "\n",
    "#### Training Progression\n",
    "\n",
    "To track progression, I recorded the number of steps, total duration, and score for each episode. Below, we will plot episodes 51 - 125 (0-50 were used to build the Memory of our model and were not trained in any capacity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('episodes_51_to_125.csv')\n",
    "\n",
    "# Filter data for episodes 51 to 125\n",
    "filtered_data = data[(data['Episode'] >= 51) & (data['Episode'] <= 125)]\n",
    "\n",
    "# Calculate cumulative steps\n",
    "filtered_data['Cumulative Steps'] = filtered_data['Steps'].cumsum()\n",
    "\n",
    "# Extract columns\n",
    "cumulative_steps = filtered_data['Cumulative Steps']\n",
    "scores = filtered_data['Score']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_steps, scores, marker='o', linestyle='-', color='b')\n",
    "plt.title(\"Cumulative Steps vs Score (Episodes 51 to 125)\")\n",
    "plt.xlabel(\"Cumulative Steps\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model takes some time to get off the ground, only beginning improvement after 50,000 timesteps. However, there is clear and visible improvement for the remaining training period. \n",
    "\n",
    "Another interesting aspect of this graph is the fact that the first trails take much less steps, they are closer together on the x axis. This is because an episode where our agent immediately loses all the points is much shorter than an episode where our agent is able to return the ball. In fact, as we train our model and as our model improves, the episodes tend to take more and more timesteps. \n",
    "\n",
    "\n",
    "#### Model Performance Visualization\n",
    "\n",
    "Next, let's go beck to the same visualization process as used for the other models. We will play a full game to completion and see what the final score is. (Reminder: our agent is on the left!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_action(self, state):\n",
    "    # Exploit: Choose the action with the highest Q-value among possible actions\n",
    "    q_values = self.model.predict(state, verbose=0)[0]  # Get Q-values\n",
    "    masked_q_values = [q_values[action] if action in self.possible_actions else -np.inf for action in range(len(q_values))]\n",
    "    best_action = np.argmax(masked_q_values)  # Select the best valid action\n",
    "    return best_action\n",
    "\n",
    "\n",
    "def play_episode_and_visualize(env, agent, render=True):\n",
    "    \"\"\"\n",
    "    Play a single game episode and visualize the environment..\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    obs = resize_frame(obs)\n",
    "\n",
    "    frame_stack = deque([obs] * 4, maxlen=4)\n",
    "    stacked_obs = np.stack(frame_stack, axis=-1)\n",
    "    stacked_obs = stacked_obs[np.newaxis, ...]\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        # Use the fixed get_action method\n",
    "        action = agent.get_action(stacked_obs)\n",
    "\n",
    "        # Take the action\n",
    "        new_obs, reward, done, info, info2 = env.step(action)\n",
    "        new_obs = resize_frame(new_obs)\n",
    "\n",
    "        frame_stack.append(new_obs)\n",
    "        stacked_obs = np.stack(frame_stack, axis=-1)\n",
    "        stacked_obs = stacked_obs[np.newaxis, ...]\n",
    "\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Render the environment\n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            plt.imshow(env.render())\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "    return score, steps\n",
    "\n",
    "    \n",
    "# Play and visualize one episode\n",
    "play_episode_and_visualize(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it wasn't perfect, But it certainly wasn't bad either. Interestingly enough, our model is both much better at defending points, but much worse at scoring points. Our previous model, the prebuilt DQN model from stable_baselines3, scored nearly every time it touched the ball but ignored it compleltly around 50% of the time. This model is more consistent and human like, it prioritizes not losing points as much as it prioritizes winning points.\n",
    "\n",
    "For further evaluation, we are going to generate the final score of 10 episodes and compare it to our pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "\n",
    "for episode in range(10):  # Evaluate over 10 episodes\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    frame_stack = deque([resize_frame(obs)] * 4, maxlen=4)  # Initialize frame stack\n",
    "    stacked_obs = np.stack(frame_stack, axis=-1)[np.newaxis, ...]  # Prepare input\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0  # Track the number of steps per episode\n",
    "\n",
    "    while not done:\n",
    "        # Predict and execute action\n",
    "        action = agent.get_action(stacked_obs)  # Use the updated get_action method\n",
    "        next_obs, reward, done, info, info2 = env.step(action)\n",
    "\n",
    "        # Update reward and preprocess next observation\n",
    "        total_reward += reward\n",
    "        next_frame = resize_frame(next_obs)\n",
    "        frame_stack.append(next_frame)\n",
    "        stacked_obs = np.stack(frame_stack, axis=-1)[np.newaxis, ...]\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Steps = {steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sense of these scores, lets look at previous scores:\n",
    "* 10 final scores generated from the untrained model that was moving randomly: [-21, -21, -20, -21, -21, -21, -20, -20, -21, -21]\n",
    "* 10 final scores generated from the model trained on stable_baselines3: [10, 1, -4, -1, 4, 14, 15, 6, -1, 4]\n",
    "* and of course, our 10 final scores from our homebuilt model: [-11, -11, -12, -19, -14, -11, -13, -12, -17, -13]\n",
    "\n",
    "Obviously, the untrained model performs signifigantly worse that the other two trained models. Additionally, while it seems the stable_baselines3 model performs the best, it's also important to note that the stable_baselines3 has had the most training time (36 hours). Additionally, in the last 30% of it's training, it began to stagnate and did not improve. The homebuilt model, on the other hand, with 28 hours of training, was still continuing to improve when I ended training (due to a time crunch). I expect that given more time, the homebuilt model would continue to improve. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Pong with TensorFlow\n",
    "\n",
    "Our whole project, up to this point, has been focused around the game of pong. Now, we are going to apply all the learning we've done to a new environment: that of the Bipedal_Walker. Documentation for the bipedal_walker can be found here: https://gymnasium.farama.org/environments/box2d/bipedal_walker/ \n",
    "\n",
    "## A. A Guilty Confession...\n",
    "\n",
    "Machine Learning is hard. When I embarked upon this project, I was certain it was properly scoped. I figured there was enough documentation that existed around OpenAI Gym that I could focus more on theory and less on implementation. Boy, was I wrong. There was barely enough documentation to properly install OpenAI Gymnasium and get the program up and running. Then, when I set out to beat Pong, I ran into complication after complication. Compiler errors. Missing dependancies. Code cells that could only be run once before I had to restarting the whole notebook. \n",
    "\n",
    "However, I perservered, and I had finally had a working model... that took 36 hours to train. For the *pre-built* model. By the time I had my tensorflow model up and running, I barely had enough time to train it a single time, and 28 hours was *still* not long enough for good results. Additionally, while I thought I'd been keeping good documentation for my write-up, it still took me many, many hours to go through and explain each of my design choices, explain the complex theory behind RL, explain each step of implementing the model, and so many other important things I've included in this report. \n",
    "\n",
    "I guess, what I am trying to say is, by the time I even began working on the Bipedal_Walker environment, it was already too late. I was out of time. However, I still tried my best. I really, really wanted this to work. And I **AM** immensely proud of my work on this project with pong and this notebook as a whole. But with the project due in an hour, I'm throwing in the towel. \n",
    "\n",
    "I still wanted to include the work I had conducted on the Bipedal_Walker environment below. Feel free to skim over it - I know this report is already way too long as it is, so I'll save you some trouble: it doesn't work. \n",
    "\n",
    "Anyway, thank you so much for understanding. This has been an incredible learning experience :) \n",
    "\n",
    "\n",
    "## B. Transitioning the Model from Pong to Bipedal_Walker\n",
    "\n",
    "#### Model Overview:\n",
    "\n",
    "The Bipedal_Walker environment is a continuous control task where a two-legged robot must learn to walk.\n",
    "\n",
    "* Game States/Observations: A 24-dimensional vector representing the robot’s joint angles, velocities, and contact sensors.\n",
    "* Actions: A 4-dimensional continuous vector controlling the torques applied to the legs and knees (range: -1 to 1).\n",
    "* Rewards: Points for forward movement, penalties for large motion and falls.\n",
    "* End Conditions: The robot either reaches the end of the terrain or falls.\n",
    "\n",
    "#### Difference between Pong and Bipedal_Walker\n",
    "\n",
    "Shifting from Pong to BipedalWalker involves signifigant changes to the model to handle a continuous control task instead of a discrete action game. While Pong relies on processing image frames with a CNN, BipedalWalker requires a fully connected network to manage vector-based observations and continuous actions. Key differences include:\n",
    "\n",
    "1. Game State:\n",
    "* Pong: Processes image frames (84x84x4) with CNN layers.\n",
    "* Bipedal_Walker: Handles a 24-dimensional vector using dense layers.\n",
    "\n",
    "2. Action Space:\n",
    "* Pong: Single actions like \"up,\" \"down,\" or \"no action.\"\n",
    "* Bipedal_Walker: Continuous actions controlling joints.\n",
    "\n",
    "3. Reward System:\n",
    "* Pong: Rewards for wins/losses infrequently, only when points are scored.\n",
    "* Bipedal_Walker: Constant rewards based on forward progress and falls.\n",
    "\n",
    "#### How we Need to Update the Model\n",
    "\n",
    "To adapt to BipedalWalker, the model will be restructured to use a fully connected architecture instead of convolutional layers. Additional changes include:\n",
    "\n",
    "1. Redesigning the input layer to accept a 24-dimensional observation vector.\n",
    "2. Modifying the output layer to produce 4 continuous action values, instead of selecting from discrete actions.\n",
    "3. Updating the training algorithm to use a proximal policy organization, which is better suited for continuous control tasks.\n",
    "\n",
    "Sources: \n",
    "* https://github.com/CSautier/BipedalWalker \n",
    "* https://medium.com/@sainijagjit/deep-deterministic-policy-gradient-ddpg-tutorial-training-a-bipedal-walker-e3c91c9cb147\n",
    "* https://en.wikipedia.org/wiki/Proximal_policy_optimization\n",
    "* https://huggingface.co/blog/deep-rl-ppo \n",
    "\n",
    "\n",
    "## C. Building our Tensorflow Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefining Our Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.states = deque(maxlen=max_len)  # Store vector observations\n",
    "        self.actions = deque(maxlen=max_len)\n",
    "        self.rewards = deque(maxlen=max_len)\n",
    "        self.done_flags = deque(maxlen=max_len)\n",
    "\n",
    "    def add_experience(self, state, reward, action, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(np.array(action))\n",
    "        self.rewards.append(reward)\n",
    "        self.done_flags.append(done)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"Sample a random batch of experiences.\"\"\"\n",
    "        indices = np.random.choice(len(self.states), batch_size, replace=False)\n",
    "        states = np.array([self.states[i] for i in indices])\n",
    "        actions = np.array([self.actions[i] for i in indices])\n",
    "        rewards = np.array([self.rewards[i] for i in indices])\n",
    "        next_states = np.array([self.next_states[i] for i in indices])\n",
    "        done_flags = np.array([self.done_flags[i] for i in indices])\n",
    "        return states, actions, rewards, next_states, done_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_new_game(name, env, agent):\n",
    "    \"\"\"\n",
    "    Initialize a new game and prepare the agent's memory.\n",
    "    We add dummy data to the agent's memory to ensure a consistent state\n",
    "    at the beginning of the game.\n",
    "    \"\"\"\n",
    "    # Reset the environment\n",
    "    state, _ = env.reset()  # Initial state is a 24-dimensional vector\n",
    "\n",
    "    # Define dummy experience\n",
    "    dummy_action = np.zeros(agent.action_dim)  # 4-dimensional zero vector\n",
    "    dummy_reward = 0.0\n",
    "    dummy_done = False\n",
    "\n",
    "    # Add experiences to memory to bootstrap the process\n",
    "    for _ in range(3):\n",
    "        agent.memory.add_experience(state, dummy_action, dummy_reward, dummy_done)\n",
    "\n",
    "def take_step(env, agent, score):\n",
    "    \"\"\"Take a single step in the environment.\"\"\"\n",
    "    state = np.array(agent.memory.states[-1])  # Most recent state\n",
    "    action = agent.get_action(state)  # Predict continuous action\n",
    "    next_state, reward, done, info, info = env.step(action)  # Execute action in the environment\n",
    "\n",
    "    # Add experience to memory\n",
    "    agent.memory.add_experience(next_state, reward, action, done)\n",
    "\n",
    "    # Train the agent if memory is sufficiently filled\n",
    "    if len(agent.memory.states) > agent.starting_mem_len:\n",
    "        agent.learn()\n",
    "\n",
    "    # Update the cumulative score\n",
    "    return score + reward, done\n",
    "\n",
    "\n",
    "\n",
    "def play_episode(name, env, agent):\n",
    "    \"\"\"\n",
    "    Play a single game episode:\n",
    "    - Reset the environment and initialize the agent.\n",
    "    - Take steps in the environment until the episode ends.\n",
    "    - Return the total score achieved in the episode.\n",
    "    \"\"\"\n",
    "    initialize_new_game(name, env, agent)\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        score, done = take_step(env, agent, score)\n",
    "\n",
    "    return score\n",
    "\n",
    "def make_env(name):\n",
    "    \"\"\"\n",
    "    Create and return a new game environment.\n",
    "    \"\"\"\n",
    "    return gym.make(name)  # No rendering required for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompiling Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, action_dim, starting_mem_len, max_mem_len, starting_epsilon, learn_rate):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.action_dim = action_dim  # Continuous action space (4 dimensions for BipedalWalker)\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = 0.9 / 100000\n",
    "        self.epsilon_min = 0.05\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = tf.keras.models.clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        # Input layer for 24-dimensional observation vector\n",
    "        model.add(Input(shape=(24,)))\n",
    "        model.add(Dense(128, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "        model.add(Dense(128, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "        model.add(Dense(64, activation=\"relu\", kernel_initializer=tf.keras.initializers.HeNormal()))\n",
    "        # Output layer for continuous actions (4 dimensions)\n",
    "        model.add(Dense(self.action_dim, activation=\"tanh\"))  # Outputs in the range [-1, 1]\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "        print(\"\\nAgent Initialized for BipedalWalker\\n\")\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Exploration vs. Exploitation\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Random exploration within the action range [-1, 1]\n",
    "            return np.random.uniform(-1, 1, size=self.action_dim)\n",
    "        else:\n",
    "            # Predict the action based on the current state\n",
    "            return self.model.predict(state, verbose=0)[0]\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Train the agent using a batch of experiences from memory.\"\"\"\n",
    "        if len(self.memory.states) < self.starting_mem_len:\n",
    "            return  # Wait until the memory is sufficiently filled\n",
    "\n",
    "        # Sample a random batch of experiences\n",
    "        batch_indices = np.random.choice(len(self.memory.states), size=32, replace=False)\n",
    "        states = np.array([self.memory.states[i] for i in batch_indices])\n",
    "        next_states = np.array([self.memory.states[i + 1] for i in batch_indices])\n",
    "        actions = [self.memory.actions[i] for i in batch_indices]  # Keep actions as a list\n",
    "        rewards = np.array([self.memory.rewards[i] for i in batch_indices])\n",
    "        done_flags = np.array([self.memory.done_flags[i] for i in batch_indices])\n",
    "\n",
    "        # Get Q-values and target Q-values\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.model_target.predict(next_states, verbose=0)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        targets = q_values.copy()\n",
    "        for i in range(32):\n",
    "            target = rewards[i]\n",
    "            if not done_flags[i]:\n",
    "                target += self.gamma * np.max(next_q_values[i])\n",
    "            targets[i] = target  # Update the Q-value for the chosen action\n",
    "\n",
    "        # Train the model\n",
    "        states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        targets_tensor = tf.convert_to_tensor(targets, dtype=tf.float32)\n",
    "        self.model.fit(states_tensor, targets_tensor, batch_size=32, verbose=0)\n",
    "\n",
    "        # Update epsilon for exploration-exploitation trade-off\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.learns += 1\n",
    "\n",
    "        # Periodically update the target network\n",
    "        if self.learns % 1000 == 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            print('\\nTarget model updated')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Box2D\n",
    "\n",
    "# Environment name\n",
    "name = \"BipedalWalker-v3\"  # Update to the new environment\n",
    "\n",
    "# Agent initialization\n",
    "action_dim = 4  # Continuous actions for BipedalWalker\n",
    "agent = Agent(\n",
    "    action_dim=24,\n",
    "    starting_mem_len=1000,  # Start learning with fewer samples for continuous control\n",
    "    max_mem_len=100000,  # Memory limit for BipedalWalker\n",
    "    starting_epsilon=1.0,  # Start with high exploration\n",
    "    learn_rate=0.0003,  # Adjusted learning rate for continuous action space\n",
    ")\n",
    "env = make_env(name)\n",
    "\n",
    "# Metrics for tracking\n",
    "scores = deque(maxlen=100)\n",
    "max_score = -float('inf')  # Initialize with negative infinity for continuous environments\n",
    "\n",
    "# Training loop\n",
    "for episode in range(5000):  # Train for a large number of episodes\n",
    "    timesteps = agent.total_timesteps  # Track steps in this episode\n",
    "    start_time = time.time()  # Time the episode\n",
    "    \n",
    "    # Play one episode\n",
    "    score = play_episode(name, env, agent)\n",
    "    scores.append(score)\n",
    "    \n",
    "    # Update the maximum score achieved\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "    \n",
    "    # Logging results\n",
    "    print(f'\\nEpisode: {episode}')\n",
    "    print(f'Steps in Episode: {agent.total_timesteps - timesteps}')\n",
    "    print(f'Duration: {time.time() - start_time:.2f} seconds')\n",
    "    print(f'Score: {score:.2f}')\n",
    "    print(f'Max Score: {max_score:.2f}')\n",
    "    print(f'Epsilon: {agent.epsilon:.4f}')\n",
    "\n",
    "    # Periodically save weights\n",
    "    if episode % 100 == 0:  # Save every 100 episodes\n",
    "        agent.model.save_weights('recent.weights.h5')\n",
    "        print('\\nWeights saved!')\n",
    "\n",
    "# Save the final model weights\n",
    "agent.model.save_weights('final.weights.h5')\n",
    "print('\\nFinal weights saved!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Conclusions and Reflection\n",
    "\n",
    "## A. Conclusions\n",
    "\n",
    "This project has been an great first dive into reinforcement learning. Watching\n",
    "the models evolve—from the untrained version to the stable_baselines3 and my\n",
    "homebuilt model—has been super rewarding. Building my own Deep Q-Learning\n",
    "network taught me so much about reinforcement learning, from understanding\n",
    "the Bellman equation to implementing experience replay and Q-value\n",
    "optimization. Since I only successfully trained Pong models, my conclusions will\n",
    "stick to those models.  \n",
    "\n",
    "The untrained model, as expected, performed poorly, with scores consistently\n",
    "around -21. The stable_baselines3 model, after 36 hours of training, performed\n",
    "the best overall, with scores ranging from -4 to 15, though it started to stagnate\n",
    "toward the end of training. My homebuilt model, trained for 28 hours, achieved\n",
    "scores between -19 and -11. While it didn’t beat the stable_baselines3 model\n",
    "during this project, it was still improving when I had to stop training due to time\n",
    "constraints. I believe that with more training, it will likely surpass the prebuilt\n",
    "model.\n",
    "\n",
    "## B. Reflection \n",
    "\n",
    "I know there is an additional requirement to submit a reflection, so I'll keep it\n",
    "short. This class was signifigantly more difficult than any other class I've taken at\n",
    "Olin. It pushed me to my limit, over and over and over again. And because of\n",
    "that, I've learned more in taking this class than any other at Olin. It really was a\n",
    "joy to work on this project, no matter how hard or frustrating it was at times. I'm\n",
    "really glad I chose to challenge myself with this class. It's been an amazing\n",
    "experience.  \n",
    "\n",
    "Specifically regarding this project, I'm so proud of what I've done. I've put hours\n",
    "and hours and hours into this project, and I think it's all been worth it. Slowly\n",
    "watching each part come together as I got a better understanding of tensorflow\n",
    "and reinforcement learning as a whole. It was incredibly satisifying to watch a\n",
    "model I'd trained beat the hardcoded bot for the first time. My one regret is that I\n",
    "wish I had more time to work on it: more time to train pong, more time to learn\n",
    "tensorflow, more time to explore other gymnasium environments.  \n",
    "\n",
    "Well, for now, I certaintly need a break from this project for a bit. But who\n",
    "knows? Maybe I'll be back one day to finally get Bipedal_Walker up off it's feet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
